{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This script downloads and processes the entire dataset\"\"\"\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from prompt_systematic_review.download_arxiv_query import query_archive\n",
    "from prompt_systematic_review.run_semantic_scholar import query_semantic_scholar\n",
    "\n",
    "# from prompt_systematic_review.arxiv_source\n",
    "# from prompt_systematic_review.semantic_scholar_source import\n",
    "import pandas as pd\n",
    "\n",
    "from prompt_systematic_review.utils import process_paper_title\n",
    "\n",
    "import openai\n",
    "import tqdm\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(dotenv_path=\"../.env\")  # load all entries from .env file\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download CSV of arXiv results\n",
    "arxiv_df = query_archive(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean arXiv CSV\n",
    "arxiv_df[\"title\"] = arxiv_df[\"title\"].apply(lambda x: process_paper_title(x))\n",
    "arxiv_df[\"source\"] = \"arXiv\"\n",
    "len(arxiv_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download CSV of Semantic Scholar\n",
    "semantic_scholar_df = query_semantic_scholar(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean Semantic CSV\n",
    "semantic_scholar_df[\"title\"] = semantic_scholar_df[\"title\"].apply(\n",
    "    lambda x: process_paper_title(x)\n",
    ")\n",
    "semantic_scholar_df[\"source\"] = \"Semantic Scholar\"\n",
    "len(semantic_scholar_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine dfs\n",
    "combined_df = pd.concat([semantic_scholar_df, arxiv_df])\n",
    "len(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deduplicate\n",
    "deduplicated_df = combined_df.drop_duplicates(subset=\"title\")\n",
    "len(deduplicated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blacklist = pd.read_csv(\"../data/blacklist.csv\")\n",
    "blacklist[\"title\"] = blacklist[\"title\"].apply(lambda x: process_paper_title(x))\n",
    "len(blacklist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deduplicated_df = deduplicated_df[~deduplicated_df[\"title\"].isin(blacklist[\"title\"])]\n",
    "len(deduplicated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code hangs at about 2801 papers\n",
    "import requests\n",
    "import os\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "def downloadPaper(url: str, title: str):\n",
    "    response = requests.get(url)\n",
    "    recurse = 0\n",
    "    while (\n",
    "        str(response.status_code) != \"200\" or len(response.content) == 0\n",
    "    ) and recurse < 5:\n",
    "        # if failed to download try again after waiting 2*recurse seconds\n",
    "        time.sleep(2 * recurse)\n",
    "        response = requests.get(url)\n",
    "        recurse += 1\n",
    "\n",
    "    if str(response.status_code) == \"200\" and len(response.content) != 0:\n",
    "        # replace invalid characters in title\n",
    "        title = process_paper_title(title=title)\n",
    "        name = title + \".pdf\"\n",
    "        with open(os.path.join(\"papers\", name), \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "\n",
    "# Assuming deduplicated_df is a pandas DataFrame with columns \"url\" and \"title\"\n",
    "data = list(zip(deduplicated_df[\"url\"].tolist(), deduplicated_df[\"title\"].tolist()))\n",
    "\n",
    "NUM_PROCESSES = 12  # adjust as needed per your machine\n",
    "with ThreadPoolExecutor(max_workers=NUM_PROCESSES) as executor:\n",
    "    executor.map(lambda p: downloadPaper(*p), data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "\n",
    "new_blacklist = []\n",
    "\n",
    "# Iterate over the files in the directory\n",
    "for filename in tqdm.tqdm(os.listdir(\"papers\")):\n",
    "    try:\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            file_path = os.path.join(\"papers\", filename)\n",
    "            with open(file_path, \"rb\") as file:\n",
    "                pdf = PyPDF2.PdfReader(file)\n",
    "                contains_prompt = False\n",
    "                for page in pdf.pages:\n",
    "                    if \"prompt\" in page.extract_text().lower():\n",
    "                        contains_prompt = True\n",
    "                        break\n",
    "\n",
    "            if not contains_prompt:\n",
    "                # Delete the file\n",
    "                os.remove(file_path)\n",
    "                # Drop the corresponding row from the dataframe\n",
    "                deduplicated_df = deduplicated_df[\n",
    "                    deduplicated_df[\"title\"] != filename[:-4]\n",
    "                ]\n",
    "                # Add the paper to the new blacklist\n",
    "                # TODO: this is messed up, results in an array of 80K single characters\n",
    "                new_blacklist += filename[:-4]\n",
    "\n",
    "    except Exception as e:\n",
    "        # Delete the file if cant be read\n",
    "        os.remove(file_path)\n",
    "        # Drop the corresponding row from the dataframe\n",
    "        deduplicated_df = deduplicated_df[deduplicated_df[\"title\"] != filename[:-4]]\n",
    "        print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "# Concatenate the old and new blacklist dataframes\n",
    "# blacklist = pd.concat([blacklist, new_blacklist], ignore_index=True)\n",
    "\n",
    "# Reset the index of the dataframe after dropping rows\n",
    "# deduplicated_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# # Save the updated blacklist dataframe\n",
    "# blacklist.to_csv('../data/blacklist.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(deduplicated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: there is smtg weird going on here...\n",
    "\n",
    "# Get a list of all the paper titles in the directory (without the .pdf extension)\n",
    "paper_titles = [\n",
    "    filename[:-4] for filename in os.listdir(\"papers\") if filename.endswith(\".pdf\")\n",
    "]\n",
    "\n",
    "# Remove any rows from deduplicated_df where the title is not in paper_titles\n",
    "deduplicated_df = deduplicated_df[deduplicated_df[\"title\"].isin(paper_titles)]\n",
    "\n",
    "len(deduplicated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the csv file\n",
    "df_for_review = pd.read_csv(\"../data/arxiv_papers_for_human_review.csv\")\n",
    "\n",
    "df_for_review[\"title\"] = df_for_review[\"title\"].apply(lambda x: process_paper_title(x))\n",
    "# Get a list of the titles in the csv file\n",
    "\n",
    "titles_for_review = df_for_review[\"title\"].tolist()\n",
    "\n",
    "# have been human reviewed as correct\n",
    "df_safe = deduplicated_df[deduplicated_df[\"title\"].isin(titles_for_review)]\n",
    "# need ai review\n",
    "df_for_ai_review = deduplicated_df[~deduplicated_df[\"title\"].isin(titles_for_review)]\n",
    "\n",
    "print(len(df_for_ai_review))\n",
    "print(len(df_safe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompt_systematic_review.automated_review import review_abstract_title_categorical\n",
    "\n",
    "results = []\n",
    "\n",
    "# Iterate over DataFrame row by row\n",
    "for index, row in tqdm.tqdm(df_for_ai_review.iterrows()):\n",
    "    # Apply function to each paper's title and abstract\n",
    "    result = review_abstract_title_categorical(\n",
    "        title=row[\"title\"],\n",
    "        abstract=row[\"abstract\"],\n",
    "        model=\"gpt-4-1106-preview\",\n",
    "    )\n",
    "    # Add result to list\n",
    "    results.append(result)\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    df_for_ai_review.loc[i, \"Probability\"] = result[\"Probability\"]\n",
    "    df_for_ai_review.loc[i, \"Reasoning\"] = result[\"Reasoning\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keepables = [\"highly relevant\", \"somewhat relevant\", \"neutral\"]\n",
    "others = [\"somewhat irrelevant\", \"highly irrelevant\"]\n",
    "\n",
    "df_ai_reviewed_positive = df_for_ai_review[\n",
    "    df_for_ai_review[\"Probability\"].isin(keepables)\n",
    "]\n",
    "df_ai_reviewed_negative = df_for_ai_review[df_for_ai_review[\"Probability\"].isin(others)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in df_ai_reviewed_negative[\"title\"]:\n",
    "#     print(i)\n",
    "df_ai_reviewed_negative.iloc[21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = pd.concat([df_safe, df_ai_reviewed_positive], ignore_index=True)\n",
    "len(df_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all the paper titles in the directory (without the .pdf extension)\n",
    "paper_titles = [\n",
    "    filename[:-4] for filename in os.listdir(\"papers\") if filename.endswith(\".pdf\")\n",
    "]\n",
    "\n",
    "# Remove any rows from deduplicated_df where the title is not in paper_titles\n",
    "df_combined = df_combined[df_combined[\"title\"].isin(paper_titles)]\n",
    "\n",
    "len(df_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get a list of all titles in df_combined\n",
    "df_titles = df_combined[\"title\"].tolist()\n",
    "c = 0\n",
    "# Iterate over all files in the \"papers\" directory\n",
    "for filename in os.listdir(\"papers\"):\n",
    "    # Check if the file is a PDF and its title is not in df_titles\n",
    "    if filename.endswith(\".pdf\") and filename[:-4] not in df_titles:\n",
    "        # Remove the file\n",
    "        os.remove(\"papers/\" + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(os.listdir(\"papers\")) == len(df_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined[\"Reasoning\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fix this\n",
    "from prompt_systematic_review.utils import auto_pipeline\n",
    "\n",
    "df_combined.to_csv(\"master_papers.csv\")\n",
    "\n",
    "auto_pipeline(\"master_papers.csv\", \"papers/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prompt_survey",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
